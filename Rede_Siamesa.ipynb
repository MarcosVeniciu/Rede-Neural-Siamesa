{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcosVeniciu/Rede-Neural-Siamesa/blob/main/Rede_Siamesa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "size = 28\n",
        "\n",
        "# Coisas pra fazer \n",
        "# 1 - passar as funções de redimensionamento, reshape, normalização(1/255)\n",
        "# Para poder gerar o dataset sem se importar com a resolução e fazer o minimo de mudanças pra vê se gasta menos memoria\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/layers/Resizing"
      ],
      "metadata": {
        "id": "vmK5covsUhNj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "LQKRgOTUUQJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções"
      ],
      "metadata": {
        "id": "0Ns0ULKYVHz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(dataset):\n",
        "  \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
        "  for anchor, positive, negative in dataset.take(1):\n",
        "    def show(ax, image):\n",
        "        ax.imshow(image, cmap=\"gray\")\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    fig = plt.figure(figsize=(9, 9))\n",
        "\n",
        "    axs = fig.subplots(3, 3)\n",
        "    for i in range(3):\n",
        "        show(axs[i, 0], anchor[0])\n",
        "        show(axs[i, 1], positive[0])\n",
        "        show(axs[i, 2], negative[0])"
      ],
      "metadata": {
        "id": "2f2PivNBW8JA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mnist"
      ],
      "metadata": {
        "id": "vBWFDxrZUTGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Eg4t8X-xS_k0"
      },
      "outputs": [],
      "source": [
        "def resize(im, nR, nC):\n",
        "    number_rows = len(im)     # source number of rows \n",
        "    number_columns = len(im[0])  # source number of columns \n",
        "    return [[ im[int(number_rows * r / nR)][int(number_columns * c / nC)]  \n",
        "                 for c in range(nC)] for r in range(nR)]\n",
        "\n",
        "\n",
        "def pre_process(img):\n",
        "  #img = np.resize(img,(size,size)) #resize(img, size, size) \n",
        "  img = np.array([img]) \n",
        "  img = img.reshape(1, size, size)\n",
        "  return img\n",
        "\n",
        "\n",
        "def dataset_keras_mnist(quantidade_imagens):\n",
        "  (x_train_origin, y_train_origin), (x_test_origin, y_test_origin) = keras.datasets.mnist.load_data()\n",
        "  del x_test_origin\n",
        "  del y_test_origin\n",
        "\n",
        "  nb_classes = 10\n",
        "  triplets=[np.zeros((quantidade_imagens, 1, size, size)) for i in range(3)]\n",
        "\n",
        "  for i in range(quantidade_imagens):\n",
        "    if i == 20000: print(\"20.000 imegens adicionadas.\")\n",
        "    if i == 25000: print(\"25.000 imegens adicionadas.\")\n",
        "    if i == 30000: print(\"30.000 imegens adicionadas.\")\n",
        "    if i == 35000: print(\"35.000 imegens adicionadas.\")\n",
        "    if i == 36000: print(\"36.000 imegens adicionadas.\")\n",
        "    if i == 37000: print(\"37.000 imegens adicionadas.\")\n",
        "    if i == 38000: print(\"38.000 imegens adicionadas.\")\n",
        "    if i == 39000: print(\"39.000 imegens adicionadas.\")\n",
        "    if i == 39900: print(\"40.000 imegens adicionadas.\")\n",
        "\n",
        "    anchor_class = np.random.randint(0, nb_classes) # Sorteia o indice de uma das classes de 0 a 9\n",
        "    nb_sample_available_for_class_AP = x_train_origin[anchor_class].shape[0] # verifica a quantidade de imagens na classe de indice anchor_class\n",
        " \n",
        "    # imagem ancora\n",
        "    while True: # sorteia um indice aleatorio ate que o indice sorteado seja igual ao indice da classe desejada\n",
        "      idx_A = np.random.randint(0, nb_sample_available_for_class_AP)\n",
        "      if y_train_origin[idx_A] == anchor_class:\n",
        "        break \n",
        "\n",
        "    # imagem positiva\n",
        "    while True: # sorteia um indice aleatorio ate que o indice sorteado seja igual ao indice da classe desejada\n",
        "      idx_P = np.random.randint(0, nb_sample_available_for_class_AP)\n",
        "      if y_train_origin[idx_P] == anchor_class:\n",
        "        break  \n",
        "    \n",
        "    # Imagem negativa\n",
        "    negative_class = (anchor_class + np.random.randint(1,nb_classes)) % nb_classes # seleciona uma outra classe para ser a negativa\n",
        "    nb_sample_available_for_class_N = x_train_origin[negative_class].shape[0]\n",
        "    while True: # sorteia um indice aleatorio ate que o indice sorteado seja igual ao indice da classe desejada\n",
        "      idx_N = np.random.randint(0, nb_sample_available_for_class_N)\n",
        "      if y_train_origin[idx_N] == negative_class:\n",
        "        break\n",
        "\n",
        "    triplets[0][i,:,:,:] = pre_process(x_train_origin[idx_A])\n",
        "    triplets[1][i,:,:,:] = pre_process(x_train_origin[idx_P])\n",
        "    triplets[2][i,:,:,:] = pre_process(x_train_origin[idx_N])\n",
        " \n",
        "  dataset = tf.data.Dataset.zip((\n",
        "  tf.data.Dataset.from_tensor_slices(triplets[0]), # anchor_dataset\n",
        "  tf.data.Dataset.from_tensor_slices(triplets[1]), # positive_dataset\n",
        "  tf.data.Dataset.from_tensor_slices(triplets[2]))) # positive_dataset\n",
        "  return dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gerar DataSet"
      ],
      "metadata": {
        "id": "Zs4mbb6KVKnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset_keras_mnist(quantidade_imagens = 40000)\n",
        "dataset = dataset.shuffle(buffer_size=1024)\n",
        "#visualize(dataset)\n",
        "\n",
        "train_ds = dataset.take(round(len(dataset) * 0.8))\n",
        "val_ds = dataset.skip(round(len(dataset) * 0.8))\n",
        "\n",
        "print(\"Total de imagens: \" + str(len(dataset)))\n",
        "print(\"   Treinamento: \" + str(len(train_ds)))\n",
        "print(\"   Validação: \" + str(len(val_ds)))\n",
        "\n",
        "\n",
        "train_ds = train_ds.batch(32, drop_remainder=False)\n",
        "val_ds = val_ds.batch(32, drop_remainder=False)\n",
        "train_ds = train_ds.prefetch(8)\n",
        "val_ds = val_ds.prefetch(8)"
      ],
      "metadata": {
        "id": "JvS_Q9wEVNEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32df62e-37ee-489b-a982-ab9f21914760"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.000 imegens adicionadas.\n",
            "25.000 imegens adicionadas.\n",
            "30.000 imegens adicionadas.\n",
            "35.000 imegens adicionadas.\n",
            "36.000 imegens adicionadas.\n",
            "37.000 imegens adicionadas.\n",
            "38.000 imegens adicionadas.\n",
            "39.000 imegens adicionadas.\n",
            "40.000 imegens adicionadas.\n",
            "Total de imagens: 40000\n",
            "   Treinamento: 32000\n",
            "   Validação: 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo"
      ],
      "metadata": {
        "id": "GGJWnE3jXTAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### PART OF THIS CODE IS USING CODE FROM VICTOR SY WANG: https://github.com/iwantooxxoox/Keras-OpenFace/blob/master/utils.py ####\n",
        "#### THIS FILE IS FROM https://github.com/shahariarrabby/deeplearning.ai/blob/master/COURSE%204%20Convolutional%20Neural%20Networks/Week%2004/Face%20Recognition/fr_utils.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from numpy import genfromtxt\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "_FLOATX = 'float32'\n",
        "\n",
        "def variable(value, dtype=_FLOATX, name=None):\n",
        "    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n",
        "    _get_session().run(v.initializer)\n",
        "    return v\n",
        "\n",
        "def shape(x):\n",
        "    return x.get_shape()\n",
        "\n",
        "def square(x):\n",
        "    return tf.square(x)\n",
        "\n",
        "def zeros(shape, dtype=_FLOATX, name=None):\n",
        "    return variable(np.zeros(shape), dtype, name)\n",
        "\n",
        "def concatenate(tensors, axis=-1):\n",
        "    if axis < 0:\n",
        "        axis = axis % len(tensors[0].get_shape())\n",
        "    return tf.concat(axis, tensors)\n",
        "\n",
        "def LRN2D(x):\n",
        "    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n",
        "\n",
        "def conv2d_bn(x,\n",
        "              layer=None,\n",
        "              cv1_out=None,\n",
        "              cv1_filter=(1, 1),\n",
        "              cv1_strides=(1, 1),\n",
        "              cv2_out=None,\n",
        "              cv2_filter=(3, 3),\n",
        "              cv2_strides=(1, 1),\n",
        "              padding=None):\n",
        "    num = '' if cv2_out == None else '1'\n",
        "    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n",
        "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
        "    tensor = Activation('relu')(tensor)\n",
        "    if padding == None:\n",
        "        return tensor\n",
        "    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n",
        "    if cv2_out == None:\n",
        "        return tensor\n",
        "    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n",
        "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
        "    tensor = Activation('relu')(tensor)\n",
        "    return tensor\n",
        "\n",
        "WEIGHTS = [\n",
        "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n",
        "  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n",
        "  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n",
        "  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n",
        "  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n",
        "  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n",
        "  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n",
        "  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n",
        "  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n",
        "  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n",
        "  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n",
        "  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n",
        "  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n",
        "  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n",
        "  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n",
        "  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n",
        "  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n",
        "  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n",
        "  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n",
        "  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n",
        "  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n",
        "  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n",
        "  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n",
        "  'dense_layer'\n",
        "]\n",
        "\n",
        "conv_shape = {\n",
        "  'conv1': [64, 3, 7, 7],\n",
        "  'conv2': [64, 64, 1, 1],\n",
        "  'conv3': [192, 64, 3, 3],\n",
        "  'inception_3a_1x1_conv': [64, 192, 1, 1],\n",
        "  'inception_3a_pool_conv': [32, 192, 1, 1],\n",
        "  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n",
        "  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n",
        "  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n",
        "  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n",
        "  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n",
        "  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n",
        "  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n",
        "  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n",
        "  'inception_3b_pool_conv': [64, 256, 1, 1],\n",
        "  'inception_3b_1x1_conv': [64, 256, 1, 1],\n",
        "  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n",
        "  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n",
        "  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n",
        "  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n",
        "  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n",
        "  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n",
        "  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n",
        "  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n",
        "  'inception_4a_pool_conv': [128, 640, 1, 1],\n",
        "  'inception_4a_1x1_conv': [256, 640, 1, 1],\n",
        "  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n",
        "  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n",
        "  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n",
        "  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n",
        "  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n",
        "  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n",
        "  'inception_5a_pool_conv': [96, 1024, 1, 1],\n",
        "  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n",
        "  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n",
        "  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n",
        "  'inception_5b_pool_conv': [96, 736, 1, 1],\n",
        "  'inception_5b_1x1_conv': [256, 736, 1, 1],\n",
        "}\n",
        "\n",
        "def load_weights_from_FaceNet(FRmodel):\n",
        "    # Load weights from csv files (which was exported from Openface torch model)\n",
        "    weights = WEIGHTS\n",
        "    weights_dict = load_weights()\n",
        "\n",
        "    # Set layer weights of the model\n",
        "    for name in weights:\n",
        "        if FRmodel.get_layer(name) != None:\n",
        "            FRmodel.get_layer(name).set_weights(weights_dict[name])\n",
        "        elif model.get_layer(name) != None:\n",
        "            model.get_layer(name).set_weights(weights_dict[name])\n",
        "\n",
        "def load_weights():\n",
        "    # Set weights path\n",
        "    dirPath = './weights'\n",
        "    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
        "    paths = {}\n",
        "    weights_dict = {}\n",
        "\n",
        "    for n in fileNames:\n",
        "        paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
        "\n",
        "    for name in WEIGHTS:\n",
        "        if 'conv' in name:\n",
        "            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
        "            conv_w = np.reshape(conv_w, conv_shape[name])\n",
        "            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
        "            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
        "            weights_dict[name] = [conv_w, conv_b]     \n",
        "        elif 'bn' in name:\n",
        "            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
        "            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
        "            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
        "            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
        "            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
        "        elif 'dense' in name:\n",
        "            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
        "            dense_w = np.reshape(dense_w, (128, 736))\n",
        "            dense_w = np.transpose(dense_w, (1, 0))\n",
        "            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
        "            weights_dict[name] = [dense_w, dense_b]\n",
        "\n",
        "    return weights_dict\n",
        "    \n",
        "def img_to_encoding(image_path, model):\n",
        "    image = cv2.imread(image_path, 1)\n",
        "    image = cv2.resize(image, (96, 96)) \n",
        "    img = image[...,::-1]\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding\n"
      ],
      "metadata": {
        "id": "ZjkTQmuHXT5m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### THIS CODE IS FROM https://github.com/shahariarrabby/deeplearning.ai/blob/master/COURSE%204%20Convolutional%20Neural%20Networks/Week%2004/Face%20Recognition/inception_blocks_v2.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from numpy import genfromtxt\n",
        "from keras import backend as K\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input #, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "#import fr_utils\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "\n",
        "def inception_block_1a(X):\n",
        "    \"\"\"\n",
        "    Implementation of an inception block\n",
        "    \"\"\"\n",
        "    \n",
        "    X_3x3 = Conv2D(96, (1, 1), data_format='channels_first', name ='inception_3a_3x3_conv1')(X)\n",
        "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name = 'inception_3a_3x3_bn1')(X_3x3)\n",
        "    X_3x3 = Activation('relu')(X_3x3)\n",
        "    X_3x3 = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_3x3)\n",
        "    X_3x3 = Conv2D(128, (3, 3), data_format='channels_first', name='inception_3a_3x3_conv2')(X_3x3)\n",
        "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_3x3_bn2')(X_3x3)\n",
        "    X_3x3 = Activation('relu')(X_3x3)\n",
        "    \n",
        "    X_5x5 = Conv2D(16, (1, 1), data_format='channels_first', name='inception_3a_5x5_conv1')(X)\n",
        "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_5x5_bn1')(X_5x5)\n",
        "    X_5x5 = Activation('relu')(X_5x5)\n",
        "    X_5x5 = ZeroPadding2D(padding=(2, 2), data_format='channels_first')(X_5x5)\n",
        "    X_5x5 = Conv2D(32, (5, 5), data_format='channels_first', name='inception_3a_5x5_conv2')(X_5x5)\n",
        "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_5x5_bn2')(X_5x5)\n",
        "    X_5x5 = Activation('relu')(X_5x5)\n",
        "\n",
        "    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n",
        "    X_pool = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3a_pool_conv')(X_pool)\n",
        "    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_pool_bn')(X_pool)\n",
        "    X_pool = Activation('relu')(X_pool)\n",
        "    X_pool = ZeroPadding2D(padding=((3, 4), (3, 4)), data_format='channels_first')(X_pool)\n",
        "\n",
        "    X_1x1 = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3a_1x1_conv')(X)\n",
        "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_1x1_bn')(X_1x1)\n",
        "    X_1x1 = Activation('relu')(X_1x1)\n",
        "   \n",
        "    # CONCAT\n",
        "    #inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n",
        "    inception = tf.concat([X_3x3, X_5x5, X_pool, X_1x1], axis=1, name=\"concat_inception_block_1a\")\n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_1b(X):\n",
        "    X_3x3 = Conv2D(96, (1, 1), data_format='channels_first', name='inception_3b_3x3_conv1')(X)\n",
        "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_3x3_bn1')(X_3x3)\n",
        "    X_3x3 = Activation('relu')(X_3x3)\n",
        "    X_3x3 = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_3x3)\n",
        "    X_3x3 = Conv2D(128, (3, 3), data_format='channels_first', name='inception_3b_3x3_conv2')(X_3x3)\n",
        "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_3x3_bn2')(X_3x3)\n",
        "    X_3x3 = Activation('relu')(X_3x3)\n",
        "\n",
        "    X_5x5 = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3b_5x5_conv1')(X)\n",
        "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_5x5_bn1')(X_5x5)\n",
        "    X_5x5 = Activation('relu')(X_5x5)\n",
        "    X_5x5 = ZeroPadding2D(padding=(2, 2), data_format='channels_first')(X_5x5)\n",
        "    X_5x5 = Conv2D(64, (5, 5), data_format='channels_first', name='inception_3b_5x5_conv2')(X_5x5)\n",
        "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_5x5_bn2')(X_5x5)\n",
        "    X_5x5 = Activation('relu')(X_5x5)\n",
        "\n",
        "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n",
        "    X_pool = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3b_pool_conv')(X_pool)\n",
        "    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_pool_bn')(X_pool)\n",
        "    X_pool = Activation('relu')(X_pool)\n",
        "    X_pool = ZeroPadding2D(padding=(4, 4), data_format='channels_first')(X_pool)\n",
        "\n",
        "    X_1x1 = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3b_1x1_conv')(X)\n",
        "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_1x1_bn')(X_1x1)\n",
        "    X_1x1 = Activation('relu')(X_1x1)\n",
        "\n",
        "    #inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n",
        "    inception = tf.concat([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n",
        "    \n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_1c(X):\n",
        "    #X_3x3 = fr_utils.conv2d_bn(X,\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_3c_3x3',\n",
        "                           cv1_out=128,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=256,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(2, 2),\n",
        "                           padding=(1, 1))\n",
        "\n",
        "    #X_5x5 = fr_utils.conv2d_bn(X,\n",
        "    X_5x5 = conv2d_bn(X,\n",
        "                           layer='inception_3c_5x5',\n",
        "                           cv1_out=32,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=64,\n",
        "                           cv2_filter=(5, 5),\n",
        "                           cv2_strides=(2, 2),\n",
        "                           padding=(2, 2))\n",
        "\n",
        "    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n",
        "    X_pool = ZeroPadding2D(padding=((0, 1), (0, 1)), data_format='channels_first')(X_pool)\n",
        "\n",
        "    #inception = concatenate([X_3x3, X_5x5, X_pool], axis=1)\n",
        "    inception = tf.concat([X_3x3, X_5x5, X_pool], axis=1)\n",
        "    \n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_2a(X):\n",
        "    #X_3x3 = fr_utils.conv2d_bn(X,\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_4a_3x3',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=192,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(1, 1),\n",
        "                           padding=(1, 1))\n",
        "    #X_5x5 = fr_utils.conv2d_bn(X,\n",
        "    X_5x5 = conv2d_bn(X,\n",
        "                           layer='inception_4a_5x5',\n",
        "                           cv1_out=32,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=64,\n",
        "                           cv2_filter=(5, 5),\n",
        "                           cv2_strides=(1, 1),\n",
        "                           padding=(2, 2))\n",
        "\n",
        "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n",
        "    #X_pool = fr_utils.conv2d_bn(X_pool,\n",
        "    X_pool = conv2d_bn(X_pool,\n",
        "                           layer='inception_4a_pool',\n",
        "                           cv1_out=128,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           padding=(2, 2))\n",
        "    #X_1x1 = fr_utils.conv2d_bn(X,\n",
        "    X_1x1 = conv2d_bn(X,\n",
        "                           layer='inception_4a_1x1',\n",
        "                           cv1_out=256,\n",
        "                           cv1_filter=(1, 1))\n",
        "    \n",
        "    #inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n",
        "    inception = tf.concat([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n",
        "    \n",
        "    return inception\n",
        "\n",
        "def inception_block_2b(X):\n",
        "    #inception4e\n",
        "    #X_3x3 = fr_utils.conv2d_bn(X,\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_4e_3x3',\n",
        "                           cv1_out=160,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=256,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(2, 2),\n",
        "                           padding=(1, 1))\n",
        "    #X_5x5 = fr_utils.conv2d_bn(X,\n",
        "    X_5x5 = conv2d_bn(X,\n",
        "                           layer='inception_4e_5x5',\n",
        "                           cv1_out=64,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=128,\n",
        "                           cv2_filter=(5, 5),\n",
        "                           cv2_strides=(2, 2),\n",
        "                           padding=(2, 2))\n",
        "    \n",
        "    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n",
        "    X_pool = ZeroPadding2D(padding=((0, 1), (0, 1)), data_format='channels_first')(X_pool)\n",
        "\n",
        "    #inception = concatenate([X_3x3, X_5x5, X_pool], axis=1)\n",
        "    inception = tf.concat([X_3x3, X_5x5, X_pool], axis=1)\n",
        "    \n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_3a(X):\n",
        "    #X_3x3 = fr_utils.conv2d_bn(X,\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_5a_3x3',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=384,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(1, 1),\n",
        "                           padding=(1, 1))\n",
        "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n",
        "    #X_pool = fr_utils.conv2d_bn(X_pool,\n",
        "    X_pool = conv2d_bn(X_pool,\n",
        "                           layer='inception_5a_pool',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           padding=(1, 1))\n",
        "    #X_1x1 = fr_utils.conv2d_bn(X,\n",
        "    X_1x1 = conv2d_bn(X,\n",
        "                           layer='inception_5a_1x1',\n",
        "                           cv1_out=256,\n",
        "                           cv1_filter=(1, 1))\n",
        "\n",
        "    #inception = concatenate([X_3x3, X_pool, X_1x1], axis=1)\n",
        "    inception = tf.concat([X_3x3, X_pool, X_1x1], axis=1)\n",
        "    \n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_3b(X):\n",
        "    #X_3x3 = fr_utils.conv2d_bn(X,\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_5b_3x3',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=384,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(1, 1),\n",
        "                           padding=(1, 1))\n",
        "    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n",
        "    #X_pool = fr_utils.conv2d_bn(X_pool,\n",
        "    X_pool = conv2d_bn(X_pool,\n",
        "                           layer='inception_5b_pool',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1))\n",
        "    X_pool = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_pool)\n",
        "\n",
        "    #X_1x1 = fr_utils.conv2d_bn(X,\n",
        "    X_1x1 = conv2d_bn(X,\n",
        "                           layer='inception_5b_1x1',\n",
        "                           cv1_out=256,\n",
        "                           cv1_filter=(1, 1))\n",
        "    \n",
        "    #inception = concatenate([X_3x3, X_pool, X_1x1], axis=1)\n",
        "    inception = tf.concat([X_3x3, X_pool, X_1x1], axis=1)\n",
        "    \n",
        "    return inception\n",
        "\n",
        "def model(input_shape):\n",
        "    \"\"\"\n",
        "    Implementation of the Inception model used for FaceNet\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "        \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "    Resize_layer = keras.layers.Resizing(96,96)\n",
        "    scale_layer  = keras.layers.Rescaling(scale=1./255)\n",
        "    X = Resize_layer(X_input)\n",
        "    X = scale_layer(X)\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X)\n",
        "    \n",
        "    # First Block\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
        "    X = BatchNormalization(axis = 1, name = 'bn1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Zero-Padding + MAXPOOL\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = MaxPooling2D((3, 3), strides = 2)(X)\n",
        "    \n",
        "    # Second Block\n",
        "    X = Conv2D(64, (1, 1), strides = (1, 1), name = 'conv2')(X)\n",
        "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn2')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Zero-Padding + MAXPOOL\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "\n",
        "    # Second Block\n",
        "    X = Conv2D(192, (3, 3), strides = (1, 1), name = 'conv3')(X)\n",
        "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Zero-Padding + MAXPOOL\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n",
        "  \n",
        "    \n",
        "    # Inception 1: a/b/c\n",
        "    X = inception_block_1a(X)\n",
        "    X = inception_block_1b(X)\n",
        "    X = inception_block_1c(X)\n",
        "    \n",
        "    # Inception 2: a/b\n",
        "    X = inception_block_2a(X)\n",
        "    X = inception_block_2b(X)\n",
        "    \n",
        "    # Inception 3: a/b\n",
        "    X = inception_block_3a(X)\n",
        "    X = inception_block_3b(X)\n",
        "    \n",
        "    # Top layer\n",
        "    X = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), data_format='channels_first')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(128, name='dense_layer')(X)\n",
        "    \n",
        "    # L2 normalization\n",
        "    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n",
        "\n",
        "    # Create model instance\n",
        "    model = Model(inputs = X_input, outputs = X, name='model')\n",
        "        \n",
        "    return model"
      ],
      "metadata": {
        "id": "V8VRSH4Xwp7L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://keras.io/examples/vision/siamese_network/\n",
        "# https://keras.io/examples/vision/siamese_contrastive/\n",
        "# https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d\n",
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "input_shape = (1, 96,96)\n",
        "\n",
        "\n",
        "class DistanceLayer(keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This layer is responsible for computing the distance between the anchor\n",
        "  embedding and the positive embedding, and the anchor embedding and the\n",
        "  negative embedding.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def call(self, anchor, positive, negative):\n",
        "    ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "    an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "    return (ap_distance, an_distance)\n",
        "\n",
        "\n",
        "\n",
        "embedding_model = model(input_shape = input_shape)\n",
        "embedding_model.summary()\n",
        "\n",
        "def Siamese_Network():\n",
        "  anchor_input = keras.layers.Input(name=\"anchor\", shape= input_shape)\n",
        "  positive_input = keras.layers.Input(name=\"positive\", shape=input_shape)\n",
        "  negative_input = keras.layers.Input(name=\"negative\", shape=input_shape)\n",
        "\n",
        "  distances = DistanceLayer()(\n",
        "      embedding_model(anchor_input),\n",
        "      embedding_model(positive_input),\n",
        "      embedding_model(negative_input),\n",
        "    )\n",
        "\n",
        "  return keras.Model(inputs=[anchor_input, positive_input, negative_input], outputs=distances, name='Siamese_Network')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "A2USxsR8ZeL3",
        "outputId": "b02785bc-5ef5-4ffc-f65b-810ff392c361"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f4b5ee5473ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-13446370e5a6>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# Define the input as a tensor with shape input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mX_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m     \u001b[0mResize_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mscale_layer\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRescaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, name, dtype, sparse, tensor, ragged, type_spec, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mtype_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     ):\n\u001b[0;32m--> 420\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0;34m\"Please provide to Input a `shape` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"or a `tensor` or a `type_spec` argument. Note that \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Please provide to Input a `shape` or a `tensor` or a `type_spec` argument. Note that `shape` does not include the batch dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseModel(keras.models.Model):\n",
        "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
        "\n",
        "    Computes the triplet loss using the three embeddings produced by the\n",
        "    Siamese Network.\n",
        "\n",
        "    The triplet loss is defined as:\n",
        "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, siamese_network, margin=0.3):\n",
        "        super().__init__()\n",
        "        self.siamese_network = siamese_network\n",
        "        self.margin = margin\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # GradientTape is a context manager that records every operation that\n",
        "        # you do inside. We are using it here to compute the loss so we can get\n",
        "        # the gradients and apply them using the optimizer specified in\n",
        "        # `compile()`.\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "\n",
        "        # Storing the gradients of the loss function with respect to the\n",
        "        # weights/parameters.\n",
        "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
        "\n",
        "        # Applying the gradients on the model using the specified optimizer\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.siamese_network.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Let's update and return the training loss metric.\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "\n",
        "        # Let's update and return the loss metric.\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        # The output of the network is a tuple containing the distances\n",
        "        # between the anchor and the positive example, and the anchor and\n",
        "        # the negative example.\n",
        "        ap_distance, an_distance = self.siamese_network(data)\n",
        "\n",
        "        # Computing the Triplet Loss by subtracting both distances and\n",
        "        # making sure we don't get a negative value.\n",
        "        loss = ap_distance - an_distance\n",
        "        loss = tf.maximum(loss + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics here so the `reset_states()` can be\n",
        "        # called automatically.\n",
        "        return [self.loss_tracker]"
      ],
      "metadata": {
        "id": "1sEtgf1BaNju"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento"
      ],
      "metadata": {
        "id": "WClf_ZErcm2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", restore_best_weights=True),\n",
        "  ]"
      ],
      "metadata": {
        "id": "Uqnl8V7qcotM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = SiameseModel(Siamese_Network())\n",
        "siamese_model.compile(optimizer=keras.optimizers.Adam(0.0001), weighted_metrics=[\"loss\"])"
      ],
      "metadata": {
        "id": "k2-oYM7wctZw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historico = siamese_model.fit(train_ds, epochs=30, callbacks=my_callbacks, validation_data = val_ds)\n",
        "embedding_model.save(\"embedding_model.h5\")"
      ],
      "metadata": {
        "id": "VV8KVU_YdHGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "eff0e1db-fbf9-44e9-dee5-54fa97ca3f0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a3f7b4e4c476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistorico\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-acf13519c647>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# `compile()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Storing the gradients of the loss function with respect to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-acf13519c647>\u001b[0m in \u001b[0;36m_compute_loss\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# between the anchor and the positive example, and the anchor and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# the negative example.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0map_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0man_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiamese_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Computing the Triplet Loss by subtracting both distances and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-14-acf13519c647>\", line 26, in train_step\n        loss = self._compute_loss(data)\n    File \"<ipython-input-14-acf13519c647>\", line 52, in _compute_loss\n        ap_distance, an_distance = self.siamese_network(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"Siamese_Network\" is incompatible with the layer: expected shape=(None, 1, 96, 96), found shape=(None, 1, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def diferenca(historico):\n",
        "  accuracy = historico.history['loss'] \n",
        "  val_accuracy = historico.history['val_loss']\n",
        "  vetor_diferenca = []\n",
        "  for i in range(len(accuracy)):\n",
        "    diferenca = abs(accuracy[i] - val_accuracy[i])\n",
        "    vetor_diferenca.append(diferenca)\n",
        "\n",
        "  return vetor_diferenca\n",
        "\n",
        "  \n",
        "def grafico(historico, nome): \n",
        "\n",
        "  plt.subplots(figsize=(25, 5)) \n",
        "  # Grafico 1: Loss\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.title( nome + ': Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Cross Entropy')\n",
        "  plt.grid()\n",
        "  plt.plot(historico.history['loss'], label='Train Loss ', color = 'blue')\n",
        "  plt.plot(historico.history['val_loss'], label='Val Loss ', color = 'red')\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  # Grafico 3: Diferença de Loss\n",
        "  vetor_diferenca = diferenca(historico)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.title(nome + \": Variação do Loss\")\n",
        "  plt.xlabel(\"Epocas\")\n",
        "  plt.ylabel(\"Diferença\")\n",
        "  plt.grid()\n",
        "  plt.plot(vetor_diferenca, color = 'red', label = 'Variação do Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  nome_save = os.path.join(\"/content\", nome + \".jpg\")\n",
        "  plt.savefig(nome_save, dpi=400,  transparent=False)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "grafico(historico, \"Rede Siamesa\")"
      ],
      "metadata": {
        "id": "GHhPSbOvdzcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste"
      ],
      "metadata": {
        "id": "DiFX1xZUagCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções"
      ],
      "metadata": {
        "id": "e9j7zvTTvfXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train_origin, y_train_origin), (x_test_origin, y_test_origin) = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "J1QsxJa1oTOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26214ec4-5be2-4ef7-ee7a-7a05d96173b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(x_test_origin):\n",
        "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
        "    print(\"Total de imagens: {0}  Resolução: {1}x{2}\".format(x_test_origin.shape[0], x_test_origin.shape[1], x_test_origin.shape[2]))\n",
        "    def show(ax, image):\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    fig = plt.figure(figsize=(9, 9))\n",
        "\n",
        "    axs = fig.subplots(3, 3)\n",
        "    for i in range(3):\n",
        "        show(axs[i, 0], x_test_origin[np.random.randint(0, 1000)])\n",
        "        show(axs[i, 1], x_test_origin[np.random.randint(0, 1000)])\n",
        "        show(axs[i, 2], x_test_origin[np.random.randint(0, 1000)])"
      ],
      "metadata": {
        "id": "Fp70tJqegdl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nome = [\"zero\", \"um\", \"dois\", \"tres\", \"quatro\", \"cinco\", \"seis\", \"sete\", \"oito\", \"nove\"]\n",
        "\n",
        "def array_to_imagem(image, novo_nome):\n",
        "  novo_nome = \"/content/imagens/\" + novo_nome + \".jpg\"\n",
        "  cv2.imwrite(novo_nome, image)\n",
        "\n",
        "def salvar_ancoras(x_test_origin, y_test_origin):\n",
        "  os.mkdir(\"imagens\")\n",
        "  \n",
        "\n",
        "  indice = 0\n",
        "  i = 0\n",
        "  while indice < 10:\n",
        "    if y_test_origin[i] == indice:\n",
        "      array_to_imagem(x_test_origin[i], nome[indice])\n",
        "      indice += 1\n",
        "    i+= 1"
      ],
      "metadata": {
        "id": "QRXcbFLscx0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ler_imagem(local):\n",
        "  return cv2.imread(local, 0)"
      ],
      "metadata": {
        "id": "N-RVQJqf1OXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_database(model):\n",
        "  database = {}\n",
        "  local = \"/content/imagens\"\n",
        "  for imagem in os.listdir(local):\n",
        "    local_imagem = os.path.join(local, imagem)\n",
        "    img = ler_imagem(local_imagem)\n",
        "    identity = os.path.splitext(os.path.basename(local_imagem))[0]\n",
        "    database[identity] = img_to_encoding(img, model)\n",
        "  \n",
        "  return database"
      ],
      "metadata": {
        "id": "YA2FBid_GzDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recognise_class(image, database, model):\n",
        "    encoding = img_to_encoding(image, model)\n",
        "    identity = None\n",
        "    min_dist = 100\n",
        "    for (name, db_enc) in database.items():\n",
        "        \n",
        "        dist = np.linalg.norm(db_enc - encoding)\n",
        "        #print('distance for %s is %s' %(name, dist))\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            identity = name\n",
        "    \n",
        "    if min_dist > 0.6:\n",
        "        return \"can't recognise.\"\n",
        "    else:\n",
        "        return str(identity)"
      ],
      "metadata": {
        "id": "iNOBlli0w9Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_to_encoding(img, model):\n",
        " # img = np.resize(image,(size,size))\n",
        "  img = np.array([img]) \n",
        "  img = img.reshape(img.shape[0], 1, size, size)\n",
        "\n",
        "  embedding = model.predict(img, verbose=0)\n",
        "  \n",
        "  return embedding"
      ],
      "metadata": {
        "id": "yEPUhahFxEkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metricas(saida):\n",
        "  certo = 0\n",
        "  errado = 0\n",
        "  incerto = 0\n",
        "  \n",
        "  for resposta in saida:\n",
        "    if resposta[1] == nome[resposta[0]]:\n",
        "      certo += 1\n",
        "    else:\n",
        "      if resposta[1] == \"can't recognise.\":\n",
        "        incerto += 1\n",
        "      else:\n",
        "        if resposta[1] != nome[resposta[0]]:\n",
        "          errado += 1\n",
        "  print()\n",
        "  print(\"Total de imagem: {0}    Certas: {1}   Erradas: {2}   Incertas: {3}\".format(len(saida), certo, errado, incerto))"
      ],
      "metadata": {
        "id": "u-3Xo4oqUHAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testes"
      ],
      "metadata": {
        "id": "p8Lj7oyPvj4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Permite visualizar algumas imagens do conjunto de teste\n",
        "visualize(x_test_origin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "hBghuIHbvq8E",
        "outputId": "d338bb5c-0e01-4470-bf86-2f5ab4c77deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de imagens: 10000  Resolução: 28x28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x648 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# salvar uma imagem de cada classe para ser a ancora\n",
        "salvar_ancoras(x_test_origin, y_test_origin)"
      ],
      "metadata": {
        "id": "0Q2KrYtawHVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para classificar um imagem como pertencente a uma das classe, o processo é dividido em 4 fasses: \\\n",
        "\n",
        "1 - uma imagem representando cada classe é codificada. \\\n",
        "\n",
        "2 - a imagem que sera classifica é codificada. \\\n",
        "\n",
        "3 - verifica a distancia da imagem codificada na fasse 2 com cada uma das imagens codificadas na fasse 1. \\\n",
        "\n",
        "4 - a imagem sera classificada de acordo com a menor distancia encontrada na fasse 3.\n"
      ],
      "metadata": {
        "id": "jtwzdkX_uFMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = tf.keras.models.load_model(\"/content/embedding_model.h5\")"
      ],
      "metadata": {
        "id": "GX_8g0g9y4no",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2be963-aaef-4545-e932-69326da71625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database = prepare_database(embedding_model)\n",
        "saida = []\n",
        "\n",
        "for i in range(15): # o maximo é 10000\n",
        "  classe = recognise_class(x_test_origin[i], database, embedding_model)\n",
        "  print(\"id {2}:: Classe Real: {0} >> Classe Prevista: {1}\".format(y_test_origin[i], classe, i))\n",
        "  saida.append([y_test_origin[i], classe])\n",
        "\n",
        "metricas(saida) "
      ],
      "metadata": {
        "id": "PBZHOZ98vGPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagens = cv2.imread(\"/content/temp.jpg\", 0)\n",
        "\n",
        "database = prepare_database(embedding_model)\n",
        "\n",
        "classe = recognise_class(imagens, database, embedding_model)\n",
        "print(classe)"
      ],
      "metadata": {
        "id": "aYo0dzIkuNQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verifica as imagem que deram errado\n",
        "cv2.imwrite(\"teste.jpg\",x_test_origin[7])"
      ],
      "metadata": {
        "id": "kfU0SIo0XY2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}